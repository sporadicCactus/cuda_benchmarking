{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pycuda.gpuarray as ga\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.autoinit\n",
    "\n",
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#host-device transfer is faster when pagelocked memory is used\n",
    "#also async doesn't work without pagelocking\n",
    "def pin(array):\n",
    "    out = cuda.pagelocked_empty_like(array)\n",
    "    out[:] = array\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Uint8ToFp32:\n",
    "    mod = SourceModule(\"\"\"\n",
    "        __global__ void uint8_to_fp32(unsigned char *x, float *y, int n_elements) {\n",
    "            int idx = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "            if (idx < n_elements) {\n",
    "                y[idx] = (float)x[idx]/255;\n",
    "            }\n",
    "        }\n",
    "    \"\"\")\n",
    "    uint8_to_fp32_func = mod.get_function(\"uint8_to_fp32\")\n",
    "    uint8_to_fp32_func.prepare(\"PPi\")\n",
    "    \n",
    "    def __init__(self, block_size=256):\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        assert x.dtype == np.uint8\n",
    "        y = ga.empty_like(x, dtype=np.float32)\n",
    "        grid_size = (x.size - 1)//self.block_size + 1\n",
    "        self.uint8_to_fp32_func.prepared_call(\n",
    "            (grid_size, 1, 1),\n",
    "            (self.block_size, 1, 1),\n",
    "            int(x.gpudata),\n",
    "            int(y.gpudata),\n",
    "            x.size\n",
    "        )\n",
    "        return y\n",
    "        \n",
    "    def call_async(self, x, stream):\n",
    "        assert x.dtype == np.uint8\n",
    "        y = ga.empty_like(x, dtype=np.float32)\n",
    "        grid_size = (x.size - 1)//self.block_size + 1\n",
    "        self.uint8_to_fp32_func.prepared_async_call(\n",
    "            (grid_size, 1, 1),\n",
    "            (self.block_size, 1, 1),\n",
    "            stream,\n",
    "            int(x.gpudata),\n",
    "            int(y.gpudata),\n",
    "            x.size\n",
    "        )\n",
    "        return y\n",
    "\n",
    "    \n",
    "#tensorrt engine wrapper for convenience\n",
    "class Module:\n",
    "    \n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        self.context = engine.create_execution_context()\n",
    "        \n",
    "        self._uint8_to_fp32 = Uint8ToFp32()\n",
    "        \n",
    "    @property\n",
    "    def max_batch_size(self):\n",
    "        return self.engine.max_batch_size\n",
    "    \n",
    "    @property\n",
    "    def input_dim3(self):\n",
    "        return self.engine.get_binding_shape(0)[1:]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        assert self.engine.get_binding_shape(0)[1:] == x.shape[1:]\n",
    "        assert x.shape[0] <= self.max_batch_size\n",
    "        assert x.dtype == np.uint8\n",
    "        \n",
    "        self.context.set_binding_shape(0, trt.Dims4(*x.shape))\n",
    "        out = ga.empty(\n",
    "            (x.shape[0], *self.engine.get_binding_shape(1)[1:]),\n",
    "            dtype=np.float32,\n",
    "            allocator = x.allocator\n",
    "        )\n",
    "        x_fp32 = self._uint8_to_fp32(x)\n",
    "        self.context.execute_v2(\n",
    "            bindings=[\n",
    "                int(x_fp32.gpudata),\n",
    "                int(out.gpudata),\n",
    "            ],\n",
    "        )\n",
    "        return out\n",
    "    \n",
    "    def call_async(self, x, stream):\n",
    "        assert self.engine.get_binding_shape(0)[1:] == x.shape[1:]\n",
    "        assert x.shape[0] <= self.max_batch_size\n",
    "        assert x.dtype == np.uint8\n",
    "        \n",
    "        self.context.set_binding_shape(0, trt.Dims4(*x.shape))\n",
    "        out = ga.empty(\n",
    "            (x.shape[0], *self.engine.get_binding_shape(1)[1:]),\n",
    "            dtype=np.float32,\n",
    "            allocator = x.allocator\n",
    "        )\n",
    "        x_fp32 = self._uint8_to_fp32.call_async(x, stream)\n",
    "        self.context.execute_async_v2(\n",
    "            bindings=[\n",
    "                int(x_fp32.gpudata),\n",
    "                int(out.gpudata),\n",
    "            ],\n",
    "            stream_handle=stream.handle\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_DIM3 = (3, 224, 224)\n",
    "MAX_BATCH_SIZE = 64\n",
    "\n",
    "def get_model_engine():\n",
    "    TRT_LOGGER = trt.Logger()\n",
    "    try:\n",
    "        with open(\"resnet18.engine\", \"rb\") as fp, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "            engine = runtime.deserialize_cuda_engine(fp.read())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        builder = trt.Builder(TRT_LOGGER)\n",
    "        network = builder.create_network(1)\n",
    "        parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "\n",
    "        try:\n",
    "            with open(\"resnet18.onnx\", 'rb') as fp:\n",
    "                parser.parse(fp.read())\n",
    "        except:\n",
    "            model = resnet18(pretrained=True)\n",
    "            torch.onnx.export(\n",
    "                model,\n",
    "                (torch.rand(1,3,224,224)),\n",
    "                \"resnet18.onnx\",\n",
    "                input_names = [\"image\"],\n",
    "                output_names = [\"logits\"],\n",
    "                dynamic_axes = {\n",
    "                    \"image\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "                    \"logits\": {0: \"batch_size\"}\n",
    "                }\n",
    "            )\n",
    "            with open(\"resnet18.onnx\", 'rb') as fp:\n",
    "                parser.parse(fp.read())\n",
    "\n",
    "        builder.max_batch_size = MAX_BATCH_SIZE\n",
    "\n",
    "        min_batch_size = 1\n",
    "        max_batch_size = MAX_BATCH_SIZE\n",
    "        opt_batch_size = MAX_BATCH_SIZE\n",
    "\n",
    "        profile = builder.create_optimization_profile()\n",
    "        profile.set_shape(\n",
    "            'image',\n",
    "            [min_batch_size,*INPUT_SHAPE_DIM3],\n",
    "            [opt_batch_size,*INPUT_SHAPE_DIM3],\n",
    "            [max_batch_size,*INPUT_SHAPE_DIM3]\n",
    "        )\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.add_optimization_profile(profile)\n",
    "\n",
    "        engine = builder.build_engine(network, config=config)\n",
    "\n",
    "        with open(\"resnet18.engine\", \"wb\") as fp:\n",
    "            fp.write(engine.serialize())\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = get_model_engine()\n",
    "model = Module(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_h = np.random.randint(0, 256, (model.max_batch_size, *model.input_dim3), dtype=np.uint8)\n",
    "x_h = pin(x_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670 µs ± 47.8 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#data transfer\n",
    "x_d = ga.to_gpu(x_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allocations are expensive, memory pools come to the rescue\n",
    "pool = cuda.DeviceMemoryPool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393 µs ± 30.1 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x_d = ga.to_gpu(x_h, allocator=pool.allocate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_transfer_rate(n_streams):\n",
    "    n_transfers_per_stream = (10_000//n_streams)\n",
    "    n_transfers = n_transfers_per_stream*n_streams\n",
    "    streams = [cuda.Stream() for _ in range(n_streams)]\n",
    "    \n",
    "    start_timestamp = time.time_ns()\n",
    "    for _ in range(n_transfers_per_stream):\n",
    "        for stream in streams:\n",
    "            ga.to_gpu_async(x_h, allocator=pool.allocate, stream=stream)\n",
    "    for stream in streams:\n",
    "        stream.synchronize()\n",
    "    end_timestamp = time.time_ns()\n",
    "    \n",
    "    transferred_bytes = x_h.size * x_h.dtype.itemsize * n_transfers\n",
    "    transfer_rate = transferred_bytes/(end_timestamp - start_timestamp)*10**9/2**20\n",
    "    print(f\"{transfer_rate:.2f} MB/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25067.79 MB/s\n"
     ]
    }
   ],
   "source": [
    "measure_transfer_rate(n_streams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25071.07 MB/s\n"
     ]
    }
   ],
   "source": [
    "measure_transfer_rate(n_streams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25071.21 MB/s\n"
     ]
    }
   ],
   "source": [
    "measure_transfer_rate(n_streams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_d = ga.to_gpu(x_h, allocator=pool.allocate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2 ms ± 6.32 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#no transfer to gpu\n",
    "y_d = model(x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 ms ± 8.23 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#with transfer to gpu\n",
    "x_d = ga.to_gpu(x_h, allocator=pool.allocate)\n",
    "y_d = model(x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 ms ± 92 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#successive model calls\n",
    "for _ in range(10):\n",
    "    y_d = model(x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 ms ± 93.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#successive transfers and model calls\n",
    "for _ in range(10):\n",
    "    x_d  = ga.to_gpu(x_h, allocator=pool.allocate)\n",
    "    y_d = model(x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = [cuda.Stream() for _ in range(2)]\n",
    "models = [Module(engine) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.3 ms ± 58.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#same ammount of work, but concurrently\n",
    "for _ in range(5):\n",
    "    for model, stream in zip(models, streams):\n",
    "        x_d = ga.to_gpu_async(x_h, allocator=pool.allocate, stream=stream)\n",
    "        y_d = model.call_async(x_d, stream)\n",
    "for stream in streams:\n",
    "    stream.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smaller batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(model.max_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_h = np.random.randint(\n",
    "    0, 256, (batch_size, *model.input_dim3), dtype=np.uint8\n",
    ")\n",
    "x_h = pin(x_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.3 ms ± 3.38 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#successive transfers and model calls\n",
    "for _ in range(10):\n",
    "    x_d  = ga.to_gpu(x_h, allocator=pool.allocate)\n",
    "    y_d = model(x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = [cuda.Stream() for _ in range(2)]\n",
    "models = [Module(engine) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.5 ms ± 17.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#same ammount of work, but concurrently\n",
    "for _ in range(5):\n",
    "    for model, stream in zip(models, streams):\n",
    "        x_d = ga.to_gpu_async(x_h, allocator=pool.allocate, stream=stream)\n",
    "        y_d = model.call_async(x_d, stream)\n",
    "for stream in streams:\n",
    "    stream.synchronize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trt",
   "language": "python",
   "name": "trt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
